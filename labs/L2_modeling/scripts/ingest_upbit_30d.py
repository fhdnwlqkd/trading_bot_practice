#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Upbit OHLCV ìˆ˜ì§‘ (XRP/KRW 1m, ìµœê·¼ Nì¼) â†’ CSV ì €ì¥(+í’ˆì§ˆ ë¡œê·¸)
- --source ccxt        : OHLCV 6ì—´
- --source upbit-native: (ë¯¸êµ¬í˜„, í™•ì¥ ìë¦¬ë§Œ)
- --with-ticks         : (ë¯¸ì‚¬ìš©; í™•ì¥ ìë¦¬ë§Œ)
"""
import os, time, argparse, logging, requests  # requestsëŠ” ì¶”í›„ í™•ì¥ ëŒ€ë¹„ë¡œë§Œ ì‚¬ìš©
from typing import List
import pandas as pd
from datetime import datetime, timezone
import ccxt

# ----------------------
# ë¡œê¹…
# ----------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("ingest.log")]
)

# ----------------------
# í’ˆì§ˆ/ì €ì¥ ìœ í‹¸
# ----------------------
def quality_check(df: pd.DataFrame, label: str = "candles") -> None:
    """ë°ì´í„°í”„ë ˆì„ í’ˆì§ˆ ë¡œê·¸(í–‰ìˆ˜/ë²”ìœ„/ì¤‘ë³µ/ê²°ì¸¡)"""
    rows = len(df)
    if rows == 0:
        logging.warning(f"[QC][{label}] No data (0 rows).")
        return
    span = (df.index.min(), df.index.max())
    dups = int(df.index.duplicated().sum())
    miss = int(df.isna().any(axis=1).sum())
    logging.info(f"[QC][{label}] rows={rows}  span={span[0]}â†’{span[1]}  dup_idx={dups}  missing_rows={miss}")

def save_csv(df: pd.DataFrame, out_path: str) -> None:
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    out = df.reset_index().rename(columns={"ts": "timestamp"})
    out.to_csv(out_path, index=False)
    logging.info(f"Saved CSV -> {out_path}")
    
    
#-----------------------
# ê²°ì¸¡ ê°ì§€ ë° ë‹¤ì‹œ ì±„ìš°ê¸°
#-----------------------
# ==== (1) ìœ í‹¸: ê²°ì¸¡ êµ¬ê°„ ì°¾ê¸° ====
def find_missing_segments(df: pd.DataFrame, col: str = "close") -> list[tuple[pd.Timestamp, pd.Timestamp]]:
    """closeê°€ NaNì¸ ì—°ì† êµ¬ê°„ì„ [(start_ts, end_ts), ...]ë¡œ ë°˜í™˜ (ëª¨ë‘ UTC, 1min ê²©ì ê°€ì •)"""
    mask = df[col].isna()
    if mask.sum() == 0:
        return []
    idx = df.index[mask]
    # ì—°ì† êµ¬ê°„ìœ¼ë¡œ ë¬¶ê¸°
    groups = (idx.to_series().diff() != pd.Timedelta("1min")).cumsum()
    segs = []
    for _, g in idx.to_series().groupby(groups):
        segs.append((g.iloc[0], g.iloc[-1]))
    return segs

# ==== (1) ìœ í‹¸: ë²”ìœ„ ì¬ìˆ˜ì§‘(CCXT) ====
def fetch_range_ccxt(symbol: str, timeframe: str,
                     start_ts: pd.Timestamp, end_ts: pd.Timestamp,
                     batch: int = 200, sleep_s: float = 0.20) -> pd.DataFrame:
    """start~end ë²”ìœ„ë¥¼ CCXTë¡œ ì¢ê²Œ ì¬ìˆ˜ì§‘ â†’ 1ë¶„ ê²©ì ë¦¬í„´(UTC index)"""
    ex = ccxt.upbit()
    start_ms = int(start_ts.timestamp() * 1000)
    end_ms   = int(end_ts.timestamp()   * 1000)
    all_rows = []
    last_ts = None
    since = start_ms
    while since <= end_ms + 60_000:  # ë ë¶„ í¬í•¨ ì—¬ìœ 
        rows = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=batch)
        if not rows:
            break
        rows = sorted(rows, key=lambda r: r[0])
        new_last = rows[-1][0]
        if last_ts is not None and new_last <= last_ts:
            break
        all_rows.extend(rows)
        last_ts = new_last
        if last_ts >= end_ms:
            break
        since = last_ts + 60_000
        time.sleep(sleep_s)

    if not all_rows:
        return pd.DataFrame(columns=["open","high","low","close","volume"],
                            index=pd.DatetimeIndex([], tz="UTC", name="ts"))

    df_new = pd.DataFrame(all_rows, columns=["ts","open","high","low","close","volume"]).drop_duplicates("ts")
    df_new["ts"] = pd.to_datetime(df_new["ts"], unit="ms", utc=True)
    df_new = df_new.set_index("ts").sort_index()

    # ìš”ì²­ë²”ìœ„ë¥¼ 1ë¶„ ê²©ìë¡œ ë§ì¶˜ ë·°ë§Œ ë°˜í™˜
    grid = pd.date_range(start=start_ts.floor("min"), end=end_ts.floor("min"), freq="1min", tz="UTC")
    df_new = df_new.reindex(grid)  # ê²°ì¸¡ì€ NaN (ì›ìë£Œ ë³´ì¡´)
    return df_new

# ==== (1) ë°±í•„ ë³¸ì²´ ====
def backfill_missing_ccxt(df: pd.DataFrame, symbol: str, timeframe: str,
                          batch: int = 200, sleep_s: float = 0.20,
                          max_passes: int = 2, pad_minutes: int = 3) -> pd.DataFrame:
    """
    ê²°ì¸¡ êµ¬ê°„ë§Œ CCXTë¡œ ì¬ìˆ˜ì§‘í•´ ë®ì–´ì”€. max_passes íšŒ ë°˜ë³µí•˜ë©° ì¤„ì–´ë“¤ë©´ ê³„ì†.
    pad_minutes: ê° êµ¬ê°„ ì•ë’¤ ë²„í¼(ë¶„)
    """
    base_cols = ["open","high","low","close","volume"]
    df_out = df.copy()
    for p in range(1, max_passes+1):
        segs = find_missing_segments(df_out, col="close")
        if not segs:
            logging.info(f"[backfill] no missing segments; stop (pass #{p})")
            break
        logging.info(f"[backfill] pass #{p} segments={len(segs)}")
        before = int(df_out["close"].isna().sum())

        for (s, e) in segs:
            s_pad = (s - pd.Timedelta(minutes=pad_minutes)).floor("min")
            e_pad = (e + pd.Timedelta(minutes=pad_minutes)).ceil("min")
            try:
                patch = fetch_range_ccxt(symbol, timeframe, s_pad, e_pad, batch=batch, sleep_s=sleep_s)
            except Exception as ex:
                logging.warning(f"[backfill] fetch error {s}~{e}: {ex}")
                continue
            # í•´ë‹¹ êµ¬ê°„ êµì§‘í•©ì— ëŒ€í•´ ë®ì–´ì“°ê¸°
            idx = patch.index
            if len(idx) == 0:
                continue
            df_out.loc[idx, base_cols] = patch[base_cols].values

        after = int(df_out["close"].isna().sum())
        logging.info(f"[backfill] pass #{p} missing_rows {before} -> {after}")
        if after >= before:
            logging.info("[backfill] no improvement; stop")
            break
    return df_out

# ----------------------
# CCXT ìˆ˜ì§‘ (OHLCVë§Œ)
# ----------------------
def fetch_ohlcv_ccxt(symbol: str, timeframe: str, minutes: int, batch: int = 200, sleep_s: float = 0.20) -> pd.DataFrame:
    ex = ccxt.upbit()
    end_ms = int(datetime.now(timezone.utc).timestamp() * 1000)
    since = end_ms - minutes * 60_000
    all_rows: List[List[float]] = []
    last_ts = None

    while since < end_ms:
        try:
            rows = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=batch)
            if not rows:
                logging.info("No more data from exchange; break.")
                break
        except ccxt.NetworkError as e:
            logging.warning(f"NetworkError: {e}; retry in 1s")
            time.sleep(1.0); continue
        except ccxt.ExchangeError as e:
            logging.error(f"ExchangeError: {e}; stop"); break

        rows = sorted(rows, key=lambda r: r[0])
        new_last_ts = rows[-1][0]
        if last_ts is not None and new_last_ts <= last_ts:
            logging.warning("Timestamp not advancing; break to avoid loop.")
            break

        all_rows.extend(rows)
        last_ts = new_last_ts
        since = last_ts + 60_000  # ë‹¤ìŒ 1ë¶„
        time.sleep(sleep_s)

    df = pd.DataFrame(all_rows, columns=["ts","open","high","low","close","volume"])
    df = df.drop_duplicates("ts").copy()
    df["ts"] = pd.to_datetime(df["ts"], unit="ms", utc=True)
    df = df.set_index("ts").sort_index()

    # ìˆ˜ì§‘ ë²”ìœ„ë¥¼ 1ë¶„ ê²©ìë¡œ ë¦¬ì¸ë±ìŠ¤(ê²°ì¸¡ì€ NaN ìœ ì§€)
    if not df.empty:
        start_dt, end_dt = df.index.min().floor("min"), df.index.max().floor("min")
        full = pd.date_range(start=start_dt, end=end_dt, freq="1min", tz="UTC")
        df = df.reindex(full)
    return df

# ----------------------
# (í™•ì¥ ìë¦¬) ì—…ë¹„íŠ¸ ë„¤ì´í‹°ë¸Œ â€” ì•„ì§ ë¯¸êµ¬í˜„
# ----------------------
def fetch_ohlcv_upbit_native(symbol: str, minutes: int, batch: int = 200, sleep_s: float = 0.12) -> pd.DataFrame:
    logging.warning("[upbit-native] not implemented yet. Returning empty DataFrame.")
    # ë¹ˆ DataFrameì„ ë°˜í™˜í•´ë„ run()ì—ì„œ ì•ˆì „ ì¢…ë£Œë˜ë„ë¡ ì„¤ê³„
    return pd.DataFrame(index=pd.DatetimeIndex([], tz="UTC"), columns=["open","high","low","close","volume"])

# ----------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ----------------------
def run(source: str, symbol: str, timeframe: str, days: int, out_csv: str, batch: int,
        with_ticks: bool, backfill: bool):
    minutes = days * 24 * 60
    logging.info(f"Start ingest: src={source}, symbol={symbol}, tf={timeframe}, days={days}, minutes={minutes}")

    if source == "upbit-native":
        df = fetch_ohlcv_upbit_native(symbol, minutes, batch=batch)  # í˜„ì¬ ë¯¸êµ¬í˜„ placeholder
    else:
        df = fetch_ohlcv_ccxt(symbol, timeframe, minutes, batch=batch)

    quality_check(df, label=source)
    if df.empty:
        logging.error("No candle data fetched. Aborting.")
        return

    # ğŸ” ì„ íƒ: ê²°ì¸¡ êµ¬ê°„ ë°±í•„ (CCXT ê²½ë¡œì—ì„œë§Œ ë™ì‘ ê¶Œì¥)
    if backfill and source == "ccxt":
        df = backfill_missing_ccxt(df, symbol, timeframe, batch=batch)
        quality_check(df, label=f"{source}+backfilled")

    save_csv(df, out_csv)
    logging.info("Done.")

# ----------------------
# CLI
# ----------------------
def main():
    ap = argparse.ArgumentParser(description="Fetch Upbit OHLCV to CSV")
    ap.add_argument("--source", choices=["ccxt","upbit-native"], default="ccxt", help="Data source (ccxt recommended for now)")
    ap.add_argument("--symbol", default="XRP/KRW")
    ap.add_argument("--timeframe", default="1m")
    ap.add_argument("--days", type=int, default=30)
    ap.add_argument("--csv", default="/Users/m2nsteel/trading_bot_practice/labs/L2_modeling/data/raw/xrp_1m_30d.csv")
    ap.add_argument("--batch", type=int, default=200)
    ap.add_argument("--with-ticks", action="store_true", help="(reserved) include trade counts from ticks")
    ap.add_argument("--backfill", action="store_true", help="ê²°ì¸¡ êµ¬ê°„ì„ CCXTë¡œ ì¬ìˆ˜ì§‘í•´ ë³´ì •")
    args = ap.parse_args()
    run(args.source, args.symbol, args.timeframe, args.days, args.csv, args.batch, args.with_ticks, args.backfill)
if __name__ == "__main__":
    main()